{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# Only read up to 4096 bytes, > 4096 has 100% malware rate\n",
    "MAX_SIZE = 4096\n",
    "TOTAL_ROWS =  113636\n",
    "TOTAL_ROWS = 20000\n",
    "USE_COLS = list(range(2, MAX_SIZE))\n",
    "ROWS = TOTAL_ROWS\n",
    "\n",
    "train = pd.read_csv(\"./data/train/train.zip\", nrows=ROWS, usecols=USE_COLS, header=None, names = list(range(0, MAX_SIZE)), error_bad_lines=False)\n",
    "train_label = pd.read_csv(\"./data/train_label.csv\", usecols=[1], nrows=ROWS)\n",
    "\n",
    "train = train.fillna(0, downcast='infer')\n",
    "assert train.shape[0] == train_label.shape[0], \"Train and label shapes are different\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import *\n",
    "from keras import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "mask = np.random.rand(len(train)) < 0.8\n",
    "\n",
    "train_data = train.values\n",
    "train_labels = train_label.values\n",
    "\n",
    "x_train = train_data[mask]\n",
    "y_train = train_labels[mask]\n",
    "x_test = train_data[~mask]\n",
    "y_test = train_labels[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4094, 32)          8192      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4094, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 131008)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33538304  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 33,591,681\n",
      "Trainable params: 33,590,721\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "16082/16082 [==============================] - 292s 18ms/step - loss: 4.2331 - acc: 0.8204\n",
      "Epoch 2/50\n",
      "16082/16082 [==============================] - 296s 18ms/step - loss: 0.7417 - acc: 0.8966\n",
      "Epoch 3/50\n",
      "16082/16082 [==============================] - 298s 19ms/step - loss: 0.6525 - acc: 0.9082\n",
      "Epoch 4/50\n",
      "16082/16082 [==============================] - 290s 18ms/step - loss: 0.6024 - acc: 0.9131\n",
      "Epoch 5/50\n",
      "16082/16082 [==============================] - 284s 18ms/step - loss: 0.5667 - acc: 0.9195\n",
      "Epoch 6/50\n",
      "16082/16082 [==============================] - 289s 18ms/step - loss: 0.5406 - acc: 0.9241\n",
      "Epoch 7/50\n",
      "16082/16082 [==============================] - 290s 18ms/step - loss: 0.5268 - acc: 0.9284\n",
      "Epoch 8/50\n",
      "16082/16082 [==============================] - 287s 18ms/step - loss: 0.5172 - acc: 0.9268\n",
      "Epoch 9/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.5020 - acc: 0.9309\n",
      "Epoch 10/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.4997 - acc: 0.9300\n",
      "Epoch 11/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.5073 - acc: 0.9294\n",
      "Epoch 12/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.4851 - acc: 0.9285\n",
      "Epoch 13/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.4765 - acc: 0.9315\n",
      "Epoch 14/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.4872 - acc: 0.9312\n",
      "Epoch 15/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.4579 - acc: 0.9360\n",
      "Epoch 16/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.5216 - acc: 0.9365\n",
      "Epoch 17/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.7412 - acc: 0.9344\n",
      "Epoch 18/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.4614 - acc: 0.9383\n",
      "Epoch 19/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.4102 - acc: 0.9415\n",
      "Epoch 20/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.4078 - acc: 0.9406\n",
      "Epoch 21/50\n",
      "16082/16082 [==============================] - 286s 18ms/step - loss: 0.4023 - acc: 0.9412\n",
      "Epoch 22/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.4119 - acc: 0.9406\n",
      "Epoch 23/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 2.3297 - acc: 0.9371\n",
      "Epoch 24/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.4807 - acc: 0.9433\n",
      "Epoch 25/50\n",
      "16082/16082 [==============================] - 286s 18ms/step - loss: 0.4371 - acc: 0.9455\n",
      "Epoch 26/50\n",
      "16082/16082 [==============================] - 286s 18ms/step - loss: 0.4412 - acc: 0.9460\n",
      "Epoch 27/50\n",
      "16082/16082 [==============================] - 286s 18ms/step - loss: 0.4469 - acc: 0.9447\n",
      "Epoch 28/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.4310 - acc: 0.9460\n",
      "Epoch 29/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.4187 - acc: 0.9475\n",
      "Epoch 30/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.4146 - acc: 0.9435\n",
      "Epoch 31/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.4262 - acc: 0.9421\n",
      "Epoch 32/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.4071 - acc: 0.9439\n",
      "Epoch 33/50\n",
      "16082/16082 [==============================] - 285s 18ms/step - loss: 0.4356 - acc: 0.9442\n",
      "Epoch 34/50\n",
      "16082/16082 [==============================] - 287s 18ms/step - loss: 0.4560 - acc: 0.9436\n",
      "Epoch 35/50\n",
      "16082/16082 [==============================] - 287s 18ms/step - loss: 0.4681 - acc: 0.9430\n",
      "Epoch 36/50\n",
      "16082/16082 [==============================] - 287s 18ms/step - loss: 0.4679 - acc: 0.9410\n",
      "Epoch 37/50\n",
      "16082/16082 [==============================] - 287s 18ms/step - loss: 0.4713 - acc: 0.9442\n",
      "Epoch 38/50\n",
      "16082/16082 [==============================] - 288s 18ms/step - loss: 0.4830 - acc: 0.9385\n",
      "Epoch 39/50\n",
      "16082/16082 [==============================] - 287s 18ms/step - loss: 0.4761 - acc: 0.9435\n",
      "Epoch 40/50\n",
      "16082/16082 [==============================] - 287s 18ms/step - loss: 0.5310 - acc: 0.9394\n",
      "Epoch 41/50\n",
      "16082/16082 [==============================] - 287s 18ms/step - loss: 0.5353 - acc: 0.9432\n",
      "Epoch 42/50\n",
      "16082/16082 [==============================] - 287s 18ms/step - loss: 0.6556 - acc: 0.9436\n",
      "Epoch 43/50\n",
      "16082/16082 [==============================] - 287s 18ms/step - loss: 0.5276 - acc: 0.9441\n",
      "Epoch 44/50\n",
      "16082/16082 [==============================] - 287s 18ms/step - loss: 0.4879 - acc: 0.9446\n",
      "Epoch 45/50\n",
      "16082/16082 [==============================] - 288s 18ms/step - loss: 0.5272 - acc: 0.9406\n",
      "Epoch 46/50\n",
      "16082/16082 [==============================] - 288s 18ms/step - loss: 0.5135 - acc: 0.9445\n",
      "Epoch 47/50\n",
      "16082/16082 [==============================] - 288s 18ms/step - loss: 0.5946 - acc: 0.9411\n",
      "Epoch 48/50\n",
      "16082/16082 [==============================] - 287s 18ms/step - loss: 0.6122 - acc: 0.9442\n",
      "Epoch 49/50\n",
      "16082/16082 [==============================] - 288s 18ms/step - loss: 0.5612 - acc: 0.9447\n",
      "Epoch 50/50\n",
      "16082/16082 [==============================] - 288s 18ms/step - loss: 0.5254 - acc: 0.9455\n",
      "Accuracy: 0.943\n"
     ]
    }
   ],
   "source": [
    "train_labels = train_label.values\n",
    "\n",
    "own_embedding_vocab_size = 256\n",
    "\n",
    "maxlen = 4094\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=own_embedding_vocab_size, # 10\n",
    "                    output_dim=32, \n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(rate=0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='elu', activity_regularizer=regularizers.l1_l2(0.0001)))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(128, activation='elu', activity_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(64, activation='elu', activity_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='elu', activity_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid', activity_regularizer=regularizers.l2(0.0001)))\n",
    "\n",
    "adam=optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['acc'])  # Compile the model\n",
    "print(model.summary())  # Summarize the model\n",
    "model.fit(x_train, y_train, epochs=50, verbose=1)  # Fit the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)  # Evaluate the model\n",
    "model.save('mlp_model.h5')\n",
    "print('Accuracy: %0.3f' % accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
